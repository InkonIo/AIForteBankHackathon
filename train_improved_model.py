"""
üéØ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ù–ê –†–ï–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• –ò–ó –ë–î
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç transactions + customer_behavior_patterns
"""

import psycopg2
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import xgboost as xgb
from pathlib import Path
import json

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ë–î
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'fraud_db',
    'user': 'postgres',
    'password': 'Alikhancool20!'
}

print("="*70)
print("üéØ –û–ë–£–ß–ï–ù–ò–ï ML-–ú–û–î–ï–õ–ò –ù–ê –†–ï–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–•")
print("="*70)

# ==================== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó –ë–î ====================

def load_training_data():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ –ë–î"""
    
    conn = psycopg2.connect(**DB_CONFIG)
    
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î...")
    
    # –ó–∞–ø—Ä–æ—Å –∫–æ—Ç–æ—Ä—ã–π —Å–æ–µ–¥–∏–Ω—è–µ—Ç transactions —Å customer_behavior_patterns
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –†–ï–ê–õ–¨–ù–´–ï —Å—Ç–æ–ª–±—Ü—ã –∏–∑ —Ç–∞–±–ª–∏—Ü—ã customer_behavior_patterns
    query = """
    SELECT 
        t.id,
        t.transaction_id,
        t.customer_id,
        t.recipient_id,
        t.amount,
        t.transaction_datetime,
        t.is_fraud,
        
        -- –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        EXTRACT(HOUR FROM t.transaction_datetime) as hour,
        EXTRACT(MINUTE FROM t.transaction_datetime) as minute,
        EXTRACT(DOW FROM t.transaction_datetime) as day_of_week,
        EXTRACT(DAY FROM t.transaction_datetime) as day_of_month,
        EXTRACT(MONTH FROM t.transaction_datetime) as month,
        
        -- –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ customer_behavior_patterns (–†–ï–ê–õ–¨–ù–´–ï —Å—Ç–æ–ª–±—Ü—ã)
        cb.avg_logins_per_day_30d,
        cb.avg_logins_per_day_7d,
        cb.avg_session_interval_sec,
        cb.burstiness_score,
        cb.exp_weighted_avg_interval,
        cb.fano_factor,
        cb.interval_zscore,
        cb.latest_os_version,
        cb.latest_phone_model,
        cb.login_freq_change_ratio,
        cb.login_ratio_7d_30d,
        cb.logins_last_30_days,
        cb.logins_last_7_days,
        cb.session_interval_std,
        cb.session_interval_variance,
        cb.unique_os_versions_30d,
        cb.unique_phone_models_30d
        
    FROM transactions t
    LEFT JOIN customer_behavior_patterns cb 
        ON t.customer_id = cb.customer_id 
        AND DATE(t.transaction_datetime) = cb.trans_date
    WHERE t.transaction_datetime >= NOW() - INTERVAL '90 days'
    """
    
    df = pd.read_sql(query, conn)
    conn.close()
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
    print(f"   –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö: {df['is_fraud'].sum()}")
    print(f"   –ß–∏—Å—Ç—ã—Ö: {(~df['is_fraud']).sum()}")
    
    return df

# ==================== FEATURE ENGINEERING ====================

def engineer_features(df):
    """–°–æ–∑–¥–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
    
    print("\nüîß Feature Engineering...")
    
    # –ö–æ–ø–∏—Ä—É–µ–º –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    df = df.copy()
    
    # 1. –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å—É–º–º—ã
    df['amount_log'] = np.log(df['amount'] + 1)
    df['amount_sqrt'] = np.sqrt(df['amount'])
    
    # 2. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df['is_night'] = ((df['hour'] >= 23) | (df['hour'] < 6)).astype(int)
    df['is_morning'] = ((df['hour'] >= 6) & (df['hour'] < 12)).astype(int)
    df['is_evening'] = ((df['hour'] >= 18) & (df['hour'] < 23)).astype(int)
    df['is_weekend'] = (df['day_of_week'].isin([5, 6])).astype(int)
    
    # 3. –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
    
    # 4. –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (OS –∏ Phone)
    # –ü—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    df['has_os_data'] = (~df['latest_os_version'].isna()).astype(int)
    df['has_phone_data'] = (~df['latest_phone_model'].isna()).astype(int)
    
    # –ó–∞–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –Ω—É–ª—è–º–∏
    df = df.fillna(0)
    
    # –£–±—Ä–∞—Ç—å inf –∑–Ω–∞—á–µ–Ω–∏—è
    df = df.replace([np.inf, -np.inf], 0)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {df.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    return df

# ==================== –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò ====================

# –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (–¢–û–õ–¨–ö–û —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤ –ë–î + —Å–æ–∑–¥–∞–Ω–Ω—ã–µ)
FEATURE_COLUMNS = [
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
    'amount', 'amount_log', 'amount_sqrt',
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    'hour', 'minute', 'day_of_week', 'day_of_month', 'month',
    'is_night', 'is_morning', 'is_evening', 'is_weekend',
    'hour_sin', 'hour_cos', 'day_sin', 'day_cos',
    
    # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∏–∑ customer_behavior_patterns)
    'avg_logins_per_day_30d',
    'avg_logins_per_day_7d',
    'avg_session_interval_sec',
    'burstiness_score',
    'exp_weighted_avg_interval',
    'fano_factor',
    'interval_zscore',
    'login_freq_change_ratio',
    'login_ratio_7d_30d',
    'logins_last_30_days',
    'logins_last_7_days',
    'session_interval_std',
    'session_interval_variance',
    'unique_os_versions_30d',
    'unique_phone_models_30d',
    'has_os_data',
    'has_phone_data'
]

def train_model(df):
    """–û–±—É—á–∏—Ç—å XGBoost –º–æ–¥–µ–ª—å"""
    
    print("\nüß† –û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏...")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
    X = df[FEATURE_COLUMNS]
    y = df['is_fraud'].astype(int)
    
    # –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"   Train: {len(X_train)} ({y_train.sum()} fraud)")
    print(f"   Test:  {len(X_test)} ({y_test.sum()} fraud)")
    
    # –°–æ–∑–¥–∞—Ç—å DMatrix
    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=FEATURE_COLUMNS)
    dtest = xgb.DMatrix(X_test, label=y_test, feature_names=FEATURE_COLUMNS)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è fraud detection)
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'max_depth': 8,
        'eta': 0.05,
        'min_child_weight': 5,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'scale_pos_weight': (len(y_train) - y_train.sum()) / y_train.sum(),  # –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
        'seed': 42
    }
    
    print(f"   Scale pos weight: {params['scale_pos_weight']:.2f}")
    
    # –û–±—É—á–µ–Ω–∏–µ —Å early stopping
    evals = [(dtrain, 'train'), (dtest, 'test')]
    
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=500,
        evals=evals,
        early_stopping_rounds=50,
        verbose_eval=50
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred_proba = model.predict(dtest)
    y_pred = (y_pred_proba >= 0.5).astype(int)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    print("\n" + "="*70)
    print("üìä –ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ò")
    print("="*70)
    
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Clean', 'Fraud']))
    
    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    print(cm)
    print(f"\nTrue Negatives:  {cm[0][0]}")
    print(f"False Positives: {cm[0][1]}")
    print(f"False Negatives: {cm[1][0]}")
    print(f"True Positives:  {cm[1][1]}")
    
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"\nROC AUC Score: {auc:.4f}")
    
    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
    print("\n" + "="*70)
    print("üéØ –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ï –ü–û–†–û–ì–ò")
    print("="*70)
    
    thresholds = find_optimal_thresholds(y_test, y_pred_proba)
    
    for name, value in thresholds.items():
        print(f"   {name:20s}: {value:.4f}")
    
    return model, thresholds

def find_optimal_thresholds(y_test, y_pred_proba):
    """–ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —Ä–∏—Å–∫–∞"""
    
    from sklearn.metrics import precision_recall_curve
    
    precision, recall, thresholds_pr = precision_recall_curve(y_test, y_pred_proba)
    
    # F1 score –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ä–æ–≥–∞
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
    
    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ F1
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds_pr[optimal_idx]
    
    print(f"\n–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ (–ø–æ F1): {optimal_threshold:.4f}")
    print(f"   Precision: {precision[optimal_idx]:.4f}")
    print(f"   Recall: {recall[optimal_idx]:.4f}")
    print(f"   F1: {f1_scores[optimal_idx]:.4f}")
    
    # –ì—Ä–∞–¥–∞—Ü–∏—è –ø–æ—Ä–æ–≥–æ–≤
    return {
        'approve_max': 0.3,  # –ù–∏–∂–µ - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–¥–æ–±—Ä–µ–Ω–∏–µ
        'verify_max': 0.5,   # –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø. –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
        'review_max': 0.7,   # –¢—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        'block_min': 0.85    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞
    }

# ==================== –°–û–•–†–ê–ù–ï–ù–ò–ï ====================

def save_model(model, thresholds):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –∏ –ø–æ—Ä–æ–≥–∏"""
    
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    # –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    Path('models').mkdir(exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å
    model.save_model('models/fraud_model.json')
    print("   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/fraud_model.json")
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä–æ–≥–∏
    with open('models/thresholds.txt', 'w') as f:
        f.write(f"APPROVE (max): {thresholds['approve_max']:.4f}\n")
        f.write(f"VERIFY (max): {thresholds['verify_max']:.4f}\n")
        f.write(f"REVIEW (max): {thresholds['review_max']:.4f}\n")
        f.write(f"BLOCK (min): {thresholds['block_min']:.4f}\n")
    
    print("   ‚úÖ –ü–æ—Ä–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: models/thresholds.txt")
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    with open('models/feature_columns.txt', 'w', encoding='utf-8') as f:
        f.write("FEATURE_COLUMNS = [\n")
        for col in FEATURE_COLUMNS:
            f.write(f"    '{col}',\n")
        f.write("]\n")
    
    print("   ‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: models/feature_columns.txt")
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
    with open('models/metrics.txt', 'w', encoding='utf-8') as f:
        f.write("–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î\n")
        f.write(f"–î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {pd.Timestamp.now()}\n")
        f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(FEATURE_COLUMNS)}\n")
    
    print("   ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: models/metrics.txt")

# ==================== MAIN ====================

def main():
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
        df = load_training_data()
        
        if len(df) < 100:
            print("\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è >100)")
            print("   –ü–æ–ø—Ä–æ–±—É–µ–º –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö...")
        
        if df['is_fraud'].sum() < 10:
            print("\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ú–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞!")
            print(f"   –ù–∞–π–¥–µ–Ω–æ {df['is_fraud'].sum()} –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è >10)")
            print("   –ü–æ–ø—Ä–æ–±—É–µ–º –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö...")
        
        # 2. Feature Engineering
        df = engineer_features(df)
        
        # 3. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å
        model, thresholds = train_model(df)
        
        # 4. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
        save_model(model, thresholds)
        
        print("\n" + "="*70)
        print("‚úÖ –ì–û–¢–û–í–û!")
        print("="*70)
        print("\nüöÄ –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ ML API —Å–µ—Ä–≤–∏—Å:")
        print("   python ml_service_improved.py")
        print("\nüìä –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()