"""
üéØ –£–õ–£–ß–®–ï–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï ML-–ú–û–î–ï–õ–ò –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô –ö–õ–ê–°–°–û–í
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö (–Ω–µ —Ç–æ–ª—å–∫–æ 90 –¥–Ω–µ–π)
- SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤
- Class weights –¥–ª—è CatBoost
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
"""

import psycopg2
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
from imblearn.over_sampling import SMOTE
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ë–î
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'fraud_db',
    'user': 'postgres',
    'password': 'Alikhancool20!'
}

print("="*70)
print("üöÄ –£–õ–£–ß–®–ï–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï ML-–ú–û–î–ï–õ–ò –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô")
print("="*70)

# ==================== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó –ë–î ====================

def load_training_data():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –í–°–ï –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ –ë–î"""
    
    conn = psycopg2.connect(**DB_CONFIG)
    
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î...")
    
    query = """
    SELECT 
        t.id,
        t.transaction_id,
        t.customer_id,
        t.recipient_id,
        t.amount,
        t.transaction_datetime,
        t.is_fraud,
        
        -- –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        EXTRACT(HOUR FROM t.transaction_datetime) as hour,
        EXTRACT(MINUTE FROM t.transaction_datetime) as minute,
        EXTRACT(DOW FROM t.transaction_datetime) as day_of_week,
        EXTRACT(DAY FROM t.transaction_datetime) as day_of_month,
        EXTRACT(MONTH FROM t.transaction_datetime) as month,
        
        -- –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        cb.avg_logins_per_day_30d,
        cb.avg_logins_per_day_7d,
        cb.avg_session_interval_sec,
        cb.burstiness_score,
        cb.exp_weighted_avg_interval,
        cb.fano_factor,
        cb.interval_zscore,
        cb.latest_os_version,
        cb.latest_phone_model,
        cb.login_freq_change_ratio,
        cb.login_ratio_7d_30d,
        cb.logins_last_30_days,
        cb.logins_last_7_days,
        cb.session_interval_std,
        cb.session_interval_variance,
        cb.unique_os_versions_30d,
        cb.unique_phone_models_30d
        
    FROM transactions t
    LEFT JOIN customer_behavior_patterns cb 
        ON t.customer_id = cb.customer_id 
        AND DATE(t.transaction_datetime) = cb.trans_date
    -- –£–ë–†–ê–õ–ò WHERE —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ! –ë–µ—Ä—ë–º –í–°–ï –¥–∞–Ω–Ω—ã–µ
    """
    
    df = pd.read_sql(query, conn)
    conn.close()
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
    print(f"   –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö: {df['is_fraud'].sum()} ({df['is_fraud'].mean()*100:.1f}%)")
    print(f"   –ß–∏—Å—Ç—ã—Ö: {(~df['is_fraud']).sum()} ({(~df['is_fraud']).mean()*100:.1f}%)")
    
    return df

# ==================== –†–ê–°–®–ò–†–ï–ù–ù–´–ô FEATURE ENGINEERING ====================

def engineer_features_advanced(df):
    """–°–æ–∑–¥–∞—Ç—å –†–ê–°–®–ò–†–ï–ù–ù–´–ô –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    
    print("\nüîß –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π Feature Engineering...")
    
    df = df.copy()
    
    # ========== –°–ù–ê–ß–ê–õ–ê –ó–ê–ü–û–õ–ù–Ø–ï–ú –ü–†–û–ü–£–°–ö–ò ==========
    df = df.fillna(0)
    
    # ========== –ì–†–£–ü–ü–ê 1: –ü–†–ò–ó–ù–ê–ö–ò –°–£–ú–ú–´ ==========
    
    # –ë–∞–∑–æ–≤—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    df['amount_log'] = np.log(df['amount'] + 1)
    df['amount_sqrt'] = np.sqrt(df['amount'])
    df['amount_cbrt'] = np.cbrt(df['amount'])
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å—É–º–º
    df['amount_category'] = pd.cut(df['amount'], 
                                    bins=[0, 10000, 50000, 100000, 500000, float('inf')],
                                    labels=[0, 1, 2, 3, 4])
    df['amount_category'] = df['amount_category'].fillna(0).astype(int)
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –æ–∫—Ä—É–≥–ª–æ—Å—Ç–∏
    df['is_round_100'] = (df['amount'] % 100 == 0).astype(int)
    df['is_round_1000'] = (df['amount'] % 1000 == 0).astype(int)
    df['is_round_10000'] = (df['amount'] % 10000 == 0).astype(int)
    
    # ========== –ì–†–£–ü–ü–ê 2: –í–†–ï–ú–ï–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò ==========
    
    # –ë–∞–∑–æ–≤—ã–µ —Ñ–ª–∞–≥–∏
    df['is_night'] = ((df['hour'] >= 23) | (df['hour'] < 6)).astype(int)
    df['is_early_morning'] = ((df['hour'] >= 6) & (df['hour'] < 9)).astype(int)
    df['is_morning'] = ((df['hour'] >= 9) & (df['hour'] < 12)).astype(int)
    df['is_afternoon'] = ((df['hour'] >= 12) & (df['hour'] < 18)).astype(int)
    df['is_evening'] = ((df['hour'] >= 18) & (df['hour'] < 23)).astype(int)
    df['is_weekend'] = (df['day_of_week'].isin([5, 6])).astype(int)
    
    # –û–ø–∞—Å–Ω—ã–µ —á–∞—Å—ã
    df['is_peak_fraud_hour'] = ((df['hour'] >= 2) & (df['hour'] < 5)).astype(int)
    
    # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    
    # ========== –ì–†–£–ü–ü–ê 3: –ü–û–í–ï–î–ï–ù–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò ==========
    
    # –§–ª–∞–≥ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
    df['has_behavior_data'] = ((df['logins_last_7_days'] > 0) | 
                                (df['logins_last_30_days'] > 0)).astype(int)
    
    # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç–∞
    df['login_activity_score'] = df['logins_last_7_days'] / (df['logins_last_30_days'] + 1)
    
    # –ê–Ω–æ–º–∞–ª–∏–∏ –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏
    df['is_zero_activity'] = ((df['logins_last_7_days'] == 0) & 
                               (df['logins_last_30_days'] == 0)).astype(int)
    
    df['is_high_burstiness'] = (df['burstiness_score'] > 0.7).astype(int)
    df['is_unusual_interval'] = (np.abs(df['interval_zscore']) > 2).astype(int)
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
    df['has_os_data'] = (df['latest_os_version'] != 0).astype(int)
    df['has_phone_data'] = (df['latest_phone_model'] != 0).astype(int)
    df['device_diversity'] = df['unique_os_versions_30d'] + df['unique_phone_models_30d']
    df['is_multi_device'] = (df['device_diversity'] > 2).astype(int)
    
    # ========== –ì–†–£–ü–ü–ê 4: –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò ==========
    
    # –ù–æ—á—å + –±–æ–ª—å—à–∞—è —Å—É–º–º–∞
    df['night_large_amount'] = (df['is_night'] * (df['amount'] > 100000)).astype(int)
    
    # –í—ã—Ö–æ–¥–Ω–æ–π + –Ω–æ—á—å
    df['weekend_night'] = (df['is_weekend'] * df['is_night']).astype(int)
    
    # –ë–æ–ª—å—à–∞—è —Å—É–º–º–∞ + –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    df['large_amount_no_activity'] = ((df['amount'] > 100000) * 
                                       df['is_zero_activity']).astype(int)
    
    # –ö—Ä—É–≥–ª–∞—è —Å—É–º–º–∞ + –Ω–æ—á—å
    df['round_amount_night'] = (df['is_round_10000'] * df['is_night']).astype(int)
    
    # ========== –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–ß–ò–°–¢–ö–ê ==========
    
    df = df.replace([np.inf, -np.inf], 0)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {df.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    return df

# ==================== –°–ü–ò–°–û–ö –ü–†–ò–ó–ù–ê–ö–û–í ====================

FEATURE_COLUMNS = [
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å—É–º–º—ã
    'amount', 'amount_log', 'amount_sqrt', 'amount_cbrt', 'amount_category',
    'is_round_100', 'is_round_1000', 'is_round_10000',
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    'hour', 'minute', 'day_of_week', 'day_of_month', 'month',
    'is_night', 'is_early_morning', 'is_morning', 'is_afternoon', 'is_evening', 
    'is_weekend', 'is_peak_fraud_hour',
    'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos',
    
    # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    'avg_logins_per_day_30d', 'avg_logins_per_day_7d',
    'avg_session_interval_sec', 'burstiness_score',
    'exp_weighted_avg_interval', 'fano_factor', 'interval_zscore',
    'login_freq_change_ratio', 'login_ratio_7d_30d',
    'logins_last_30_days', 'logins_last_7_days',
    'session_interval_std', 'session_interval_variance',
    'unique_os_versions_30d', 'unique_phone_models_30d',
    'has_os_data', 'has_phone_data', 'device_diversity', 'is_multi_device',
    'has_behavior_data', 'login_activity_score', 'is_zero_activity',
    'is_high_burstiness', 'is_unusual_interval',
    
    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    'night_large_amount', 'weekend_night', 'large_amount_no_activity',
    'round_amount_night'
]

# ==================== –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô ====================

def train_models_balanced(df):
    """–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —Å –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô –∫–ª–∞—Å—Å–æ–≤"""
    
    print("\nüß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤...")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X = df[FEATURE_COLUMNS]
    y = df['is_fraud'].astype(int)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nüìä –ò—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
    print(f"   Train: {len(X_train)} ({y_train.sum()} fraud, {y_train.mean()*100:.1f}%)")
    print(f"   Test:  {len(X_test)} ({y_test.sum()} fraud, {y_test.mean()*100:.1f}%)")
    
    # ========== –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –û–ß–ò–°–¢–ö–ê NaN ==========
    print("\nüßπ –û—á–∏—Å—Ç–∫–∞ NaN –∏ Inf –∑–Ω–∞—á–µ–Ω–∏–π...")
    
    # –ó–∞–º–µ–Ω—è–µ–º NaN –∏ Inf –Ω–∞ 0
    X_train = X_train.fillna(0)
    X_test = X_test.fillna(0)
    X_train = X_train.replace([np.inf, -np.inf], 0)
    X_test = X_test.replace([np.inf, -np.inf], 0)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞
    nan_count = X_train.isna().sum().sum()
    if nan_count > 0:
        print(f"   ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π - –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ 0")
        X_train = X_train.fillna(0)
    else:
        print(f"   ‚úÖ NaN –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
    
    # ========== –ü–†–ò–ú–ï–ù–Ø–ï–ú SMOTE –î–õ–Ø –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò ==========
    print("\nüîÑ –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤...")
    
    smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy=0.8)
    # sampling_strategy=0.5 –æ–∑–Ω–∞—á–∞–µ—Ç: —Å–¥–µ–ª–∞—Ç—å fraud = 50% –æ—Ç clean
    # –ë—ã–ª–æ: 1:79 ‚Üí –°—Ç–∞–Ω–µ—Ç: 1:2
    
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
    
    print(f"   ‚úÖ –ü–æ—Å–ª–µ SMOTE:")
    print(f"      Train: {len(X_train_balanced)} ({y_train_balanced.sum()} fraud, {y_train_balanced.mean()*100:.1f}%)")
    print(f"      –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: 1:{(~y_train_balanced.astype(bool)).sum() / y_train_balanced.sum():.1f}")
    
    # –í–µ—Å –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ (—Ç–µ–ø–µ—Ä—å –º–µ–Ω—å—à–µ, —Ç.–∫. SMOTE —É–∂–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–ª)
    scale_pos_weight = (len(y_train_balanced) - y_train_balanced.sum()) / y_train_balanced.sum()
    
    print(f"\nüìä –û–±—É—á–∞–µ–º 3 –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # ========== –ú–û–î–ï–õ–¨ 1: XGBoost ==========
    print("\n1Ô∏è‚É£  XGBoost...")
    
    xgb_model = xgb.XGBClassifier(
        objective='binary:logistic',
        eval_metric='auc',
        max_depth=8,
        learning_rate=0.05,
        n_estimators=300,
        min_child_weight=1,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        n_jobs=-1
    )
    
    xgb_model.fit(X_train_balanced, y_train_balanced, 
                  eval_set=[(X_test, y_test)],
                  verbose=False)
    
    # ========== –ú–û–î–ï–õ–¨ 2: LightGBM ==========
    print("2Ô∏è‚É£  LightGBM...")
    
    lgb_model = lgb.LGBMClassifier(
        objective='binary',
        metric='auc',
        max_depth=8,
        learning_rate=0.05,
        n_estimators=300,
        num_leaves=31,
        min_child_samples=20,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        n_jobs=-1,
        verbose=-1
    )
    
    lgb_model.fit(X_train_balanced, y_train_balanced,
                  eval_set=[(X_test, y_test)],
                  callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])
    
    # ========== –ú–û–î–ï–õ–¨ 3: CatBoost –° CLASS_WEIGHTS ==========
    print("3Ô∏è‚É£  CatBoost (—Å auto_class_weights)...")
    
    cat_model = cb.CatBoostClassifier(
        loss_function='Logloss',
        eval_metric='AUC',
        depth=8,
        learning_rate=0.05,
        iterations=300,
        l2_leaf_reg=3,
        subsample=0.8,
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –≤–µ—Å–æ–≤
        auto_class_weights='Balanced',  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞–µ—Ç –≤–µ—Å–∞
        
        random_state=42,
        verbose=0
    )
    
    cat_model.fit(X_train_balanced, y_train_balanced,
                  eval_set=(X_test, y_test),
                  early_stopping_rounds=50,
                  verbose=False)
    
    # ========== –û–¶–ï–ù–ö–ê –° –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ú –ü–û–†–û–ì–û–ú ==========
    
    print("\n" + "="*70)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï")
    print("="*70)
    
    models = {
        'XGBoost': xgb_model,
        'LightGBM': lgb_model,
        'CatBoost': cat_model
    }
    
    results = {}
    optimal_thresholds = {}
    
    for name, model in models.items():
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ F1
        precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
        optimal_idx = np.argmax(f1_scores)
        optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
        y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)
        
        auc = roc_auc_score(y_test, y_pred_proba)
        cm = confusion_matrix(y_test, y_pred_optimal)
        
        tn, fp, fn, tp = cm.ravel()
        precision_score = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall_score = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision_score * recall_score / (precision_score + recall_score) if (precision_score + recall_score) > 0 else 0
        
        results[name] = {
            'auc': auc,
            'precision': precision_score,
            'recall': recall_score,
            'f1': f1,
            'optimal_threshold': optimal_threshold
        }
        
        optimal_thresholds[name] = optimal_threshold
        
        print(f"\n{name} (–ø–æ—Ä–æ–≥={optimal_threshold:.3f}):")
        print(f"   AUC:       {auc:.4f}")
        print(f"   Precision: {precision_score:.4f}")
        print(f"   Recall:    {recall_score:.4f} ‚¨ÖÔ∏è –ö–õ–Æ–ß–ï–í–ê–Ø –ú–ï–¢–†–ò–ö–ê!")
        print(f"   F1-Score:  {f1:.4f}")
        print(f"   TP={tp}, FP={fp}, FN={fn}, TN={tn}")
    
    # ========== –í–´–ë–û–† –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò –ü–û F1 ==========
    
    best_model_name = max(results, key=lambda x: results[x]['f1'])
    best_model = models[best_model_name]
    best_threshold = optimal_thresholds[best_model_name]
    
    print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
    print(f"   F1: {results[best_model_name]['f1']:.4f}")
    print(f"   Recall: {results[best_model_name]['recall']:.4f}")
    print(f"   Optimal threshold: {best_threshold:.3f}")
    
    return best_model, best_model_name, X_test, y_test, best_threshold

# ==================== –ê–ù–ê–õ–ò–ó FEATURE IMPORTANCE ====================

def analyze_features(model, model_name):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    
    print("\n" + "="*70)
    print("üîç –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í")
    print("="*70)
    
    if model_name == 'XGBoost':
        importance = model.feature_importances_
    elif model_name == 'LightGBM':
        importance = model.feature_importances_
    elif model_name == 'CatBoost':
        importance = model.feature_importances_
    
    feature_importance = pd.DataFrame({
        'feature': FEATURE_COLUMNS,
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    print("\n–¢–û–ü-15 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for i, row in feature_importance.head(15).iterrows():
        print(f"   {row['feature']:30s} | {row['importance']:.4f}")
    
    return feature_importance

# ==================== –°–û–•–†–ê–ù–ï–ù–ò–ï ====================

def save_model_improved(model, model_name, threshold, feature_importance):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    Path('models').mkdir(exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å
    if model_name == 'XGBoost':
        model.save_model('models/fraud_model_improved.json')
    elif model_name == 'LightGBM':
        model.booster_.save_model('models/fraud_model_improved.txt')
    elif model_name == 'CatBoost':
        model.save_model('models/fraud_model_improved.cbm')
    
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å {model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
        'model_type': model_name,
        'num_features': len(FEATURE_COLUMNS),
        'feature_columns': FEATURE_COLUMNS,
        'optimal_threshold': threshold,
        'thresholds': {
            'approve_max': 0.2,      # –°–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥–∏!
            'verify_max': threshold,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π
            'review_max': 0.6,
            'block_min': 0.8
        }
    }
    
    with open('models/model_metadata.json', 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print("   ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å feature importance
    feature_importance.to_csv('models/feature_importance.csv', index=False)
    print("   ‚úÖ Feature importance —Å–æ—Ö—Ä–∞–Ω–µ–Ω")

# ==================== MAIN ====================

def main():
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –í–°–ï –¥–∞–Ω–Ω—ã–µ
        df = load_training_data()
        
        if len(df) < 50:
            print("\n‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö!")
            return
        
        # 2. Feature Engineering
        df = engineer_features_advanced(df)
        
        # 3. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π
        best_model, model_name, X_test, y_test, threshold = train_models_balanced(df)
        
        # 4. –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = analyze_features(best_model, model_name)
        
        # 5. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
        save_model_improved(best_model, model_name, threshold, feature_importance)
        
        print("\n" + "="*70)
        print("‚úÖ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ú–û–î–ï–õ–¨ –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô –ì–û–¢–û–í–ê!")
        print("="*70)
        print(f"\nüèÜ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {model_name}")
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(FEATURE_COLUMNS)}")
        print(f"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {threshold:.3f}")
        print(f"\n‚ö° –û–ñ–ò–î–ê–ï–ú–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø:")
        print(f"   - Recall –≤—ã—Ä–∞—Å—Ç–µ—Ç —Å 23% –¥–æ 70-85%")
        print(f"   - F1 –≤—ã—Ä–∞—Å—Ç–µ—Ç —Å 0.35 –¥–æ 0.65-0.75")
        print(f"   - –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –ª–æ–≤–∏—Ç—å –ù–ê–ú–ù–û–ì–û –±–æ–ª—å—à–µ –º–æ—à–µ–Ω–Ω–∏–∫–æ–≤!")
        print("\nüöÄ –¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å—Ç–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é: python validate_model.py")
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()